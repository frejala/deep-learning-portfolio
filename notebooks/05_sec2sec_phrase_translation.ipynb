{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2MZc+NqIQ7ApjeMN8hwDW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Pk1itGSzhLV","executionInfo":{"status":"ok","timestamp":1759692469259,"user_tz":180,"elapsed":10181,"user":{"displayName":"Fabrizio Rejala","userId":"01308745419046227170"}},"outputId":"12027e0c-da0d-40de-b4de-9e4dd0df06f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n","Requirement already satisfied: Matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n","Requirement already satisfied: mlxtend in /usr/local/lib/python3.12/dist-packages (0.23.4)\n","Requirement already satisfied: prefixspan in /usr/local/lib/python3.12/dist-packages (0.5.2)\n","Requirement already satisfied: scikit-fuzzy in /usr/local/lib/python3.12/dist-packages (0.5.0)\n","Requirement already satisfied: umap-learn in /usr/local/lib/python3.12/dist-packages (0.5.9.post2)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n","Requirement already satisfied: stanza in /usr/local/lib/python3.12/dist-packages (1.11.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: tesseract in /usr/local/lib/python3.12/dist-packages (0.1.3)\n","Requirement already satisfied: pytesseract in /usr/local/lib/python3.12/dist-packages (0.3.13)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n","Requirement already satisfied: tensorflow==2.19.0 in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.12/dist-packages (4.9.9)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.12/dist-packages (0.0.7)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.12/dist-packages (7.16.6)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.9.23)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (5.29.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (4.15.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.3)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.75.1)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.10.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.14.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.5.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from Matplotlib) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from Matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from Matplotlib) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from Matplotlib) (1.4.9)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from Matplotlib) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from Matplotlib) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (3.5)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.9.30)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n","Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from prefixspan) (0.6.2)\n","Requirement already satisfied: extratools>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from prefixspan) (0.8.2.1)\n","Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (0.60.0)\n","Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn) (0.5.13)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from umap-learn) (4.67.1)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (from stanza) (2.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n","Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.19.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets) (0.8.1)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets) (0.1.9)\n","Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (1.13.0)\n","Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets) (4.2.1)\n","Requirement already satisfied: promise in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets) (2.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets) (5.9.5)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets) (18.1.0)\n","Requirement already satisfied: simple_parsing in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets) (0.1.7)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets) (1.17.2)\n","Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from tensorflow-datasets) (0.10.2)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (4.13.5)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert) (6.2.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert) (0.7.1)\n","Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (5.8.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert) (0.3.0)\n","Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (3.0.3)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (3.1.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (0.10.2)\n","Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (5.10.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (1.5.1)\n","Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (2.19.2)\n","Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert) (5.7.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert) (0.5.1)\n","Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert) (1.4.0)\n","Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (0.8.1)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.5.2)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.23.0)\n","Requirement already satisfied: sortedcontainers>=1.5.10 in /usr/local/lib/python3.12/dist-packages (from extratools>=0.8.1->prefixspan) (2.4.0)\n","Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from extratools>=0.8.1->prefixspan) (0.12.1)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.7->nbconvert) (4.4.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.17.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from nbclient>=0.5.0->nbconvert) (7.4.9)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert) (2.21.2)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat>=5.7->nbconvert) (4.25.1)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.9)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.3)\n","Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert) (2.8)\n","Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree->tensorflow-datasets) (25.3.0)\n","Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.12/dist-packages (from simple_parsing->tensorflow-datasets) (0.17.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.12/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.70.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2025.9.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.27.1)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (0.4)\n","Requirement already satisfied: nest-asyncio>=1.5.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.6.0)\n","Requirement already satisfied: pyzmq>=23.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (26.2.1)\n","Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.4.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (4.0.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n"]}],"source":["!pip install numpy Pillow Matplotlib pandas seaborn scikit-learn requests scikit-image mlxtend prefixspan scikit-fuzzy umap-learn openpyxl stanza torch torchvision tesseract pytesseract nltk wordcloud spacy tensorflow==2.19.0 tensorflow-datasets opencv-python ucimlrepo nbconvert"]},{"cell_type":"code","source":["import zipfile\n","import os\n","from pathlib import Path\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import requests\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow.data as tf_data\n","import tensorflow.strings as tf_strings\n","import keras\n","from keras import layers\n","from keras import ops\n","from keras.layers import TextVectorization"],"metadata":{"id":"i9vSHDTlzqoF","executionInfo":{"status":"ok","timestamp":1759692537139,"user_tz":180,"elapsed":9234,"user":{"displayName":"Fabrizio Rejala","userId":"01308745419046227170"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# vamos a descargar nuestro dataset\n","url = 'https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip'\n","zip_path = \"./spa-eng.zip\"\n","print(\"Descargando dataset\")\n","\n","try:\n","    headers = {\n","        'User-Agent': 'Mozilla/5.0 (Linux; Android 7.0; WAS-L03T) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.101 Mobile Safari/537.36'\n","    }\n","    response = requests.get(url, headers=headers, stream=None)\n","    response.raise_for_status()\n","\n","    # guardar archivo\n","    with open(zip_path, \"wb\") as f:\n","        for chunk in response.iter_content(chunk_size=8192):\n","            if chunk:\n","                f.write(chunk)\n","    print(\"Descarga completa\")\n","\n","except requests.exceptions.RequestException as e:\n","    print(f\"Error al descargar el archivo {e}\")\n","    print(\"Intenta de nuevo\")\n","    !wget -O spa-eng.zip https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","\n","extract_path = \"./spa-eng\"\n","os.makedirs(extract_path, exist_ok=True)\n","print(\"Extrayendo el archivo\")\n","\n","try:\n","    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n","        zip_ref.extractall(extract_path)\n","except zipfile.BadZipFile:\n","    print(\"Error al extraer el archivo zip\")\n","\n","actual_data_path = os.path.join(extract_path, \"spa-eng\")\n","print(f\"Buscando datos en {actual_data_path}\")\n","\n","text_file = os.path.join(actual_data_path, \"spa.txt\")\n","print(f\"Buscando datos en {text_file}\")\n","\n","if os.path.exists(text_file):\n","    with open(text_file, encoding=\"utf-8\") as f:\n","        lines = f.read().split(\"\\n\")[:-1]\n","    text_pairs = []\n","    for line in lines:\n","        if \"\\t\" in line:\n","            parts = line.split(\"\\t\")\n","            if len(parts) >= 2:\n","                eng, spa = parts[0], parts[1]\n","                spa = \"[start]\" + spa + \"[end]\"\n","                text_pairs.append((eng, spa))\n","\n","    print(f\"Correctamente cargados {len(text_pairs)} pares traducidos\")\n","\n","    print(\"Ejemplos\")\n","    for i in range(min(5, len(text_pairs))):\n","        print(f\"English {text_pairs[i][0]}\")\n","        print(f\"Spanish {text_pairs[i][1]}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QP9YvKc2zsun","executionInfo":{"status":"ok","timestamp":1759692540601,"user_tz":180,"elapsed":1167,"user":{"displayName":"Fabrizio Rejala","userId":"01308745419046227170"}},"outputId":"f4cbd6d5-697d-4cbd-b28e-ddc79a095542"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Descargando dataset\n","Descarga completa\n","Extrayendo el archivo\n","Buscando datos en ./spa-eng/spa-eng\n","Buscando datos en ./spa-eng/spa-eng/spa.txt\n","Correctamente cargados 118964 pares traducidos\n","Ejemplos\n","English Go.\n","Spanish [start]Ve.[end]\n","\n","English Go.\n","Spanish [start]Vete.[end]\n","\n","English Go.\n","Spanish [start]Vaya.[end]\n","\n","English Go.\n","Spanish [start]Váyase.[end]\n","\n","English Hi.\n","Spanish [start]Hola.[end]\n","\n"]}]},{"cell_type":"code","source":["for _ in range(5):\n","    print(random.choice(text_pairs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RThjXPCfzunb","executionInfo":{"status":"ok","timestamp":1759692566374,"user_tz":180,"elapsed":7,"user":{"displayName":"Fabrizio Rejala","userId":"01308745419046227170"}},"outputId":"56c93524-4c96-44b6-baaf-c4fad1068022"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["('Tom got very angry at Mary.', '[start]Tom se enfadó mucho con Mary.[end]')\n","('Send him in.', '[start]Mandalo adentro.[end]')\n","('She is holding a red flower.', '[start]Ella está sosteniendo una flor roja.[end]')\n","('I have a collection of documentaries.', '[start]Tengo una colección de documentales.[end]')\n","(\"I'm going to change my shirt.\", '[start]Me voy a cambiar de camisa.[end]')\n"]}]},{"cell_type":"code","source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.15*len(text_pairs))\n","num_train_samples = len(text_pairs) -2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n","text_pairs = text_pairs[num_train_samples + num_val_samples:]"],"metadata":{"id":"tWTtIgrNzv1t","executionInfo":{"status":"ok","timestamp":1759692568294,"user_tz":180,"elapsed":71,"user":{"displayName":"Fabrizio Rejala","userId":"01308745419046227170"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print(f'{len(text_pairs)} total pairs')\n","print(f'{len(train_pairs)} training pairs')\n","print(f'{len(val_pairs)} validation pairs')\n","print(f'{len(text_pairs)} test pairs')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tAuMYgWzxrX","executionInfo":{"status":"ok","timestamp":1759692569217,"user_tz":180,"elapsed":6,"user":{"displayName":"Fabrizio Rejala","userId":"01308745419046227170"}},"outputId":"84c07b81-e0b2-4e34-9de6-0058cb22eea8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["17844 total pairs\n","83276 training pairs\n","17844 validation pairs\n","17844 test pairs\n"]}]},{"cell_type":"markdown","source":["Vectorizar los datos: TextVectorization\n","\n","Al vectorizar los datos, estamos conviertiendo esos \"textos\"a secuencias de números de enteros que sirvan para nuestro modelo"],"metadata":{"id":"GDwEMqWXz0BP"}},{"cell_type":"code","source":["strip_chars = string.punctuation\n","strin_chars = strip_chars.replace(']','')\n","strin_chars = strip_chars.replace('[','')\n","\n","vocab_size = 150000\n","sequence_length = 20\n","batch_size = 64\n","\n","def custom_standarization(input_string):\n","    lowercase = tf_strings.lower(input_string)\n","    return tf_strings.regex_replace(lowercase, '[%s]' % re.escape(strin_chars), '')\n","\n","eng_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode='int',\n","    output_sequence_length=sequence_length\n",")\n","\n","spa_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode='int',\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standarization,\n",")\n","\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","eng_vectorization.adapt(train_eng_texts)\n","spa_vectorization.adapt(train_spa_texts)"],"metadata":{"id":"CL0gpQTOz1dV","executionInfo":{"status":"ok","timestamp":1759692571134,"user_tz":180,"elapsed":872,"user":{"displayName":"Fabrizio Rejala","userId":"01308745419046227170"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Formateo de nuestro dataset: tuplas - input (encoder y decoder) / target (lo que nuestro modelo trata de predecir)"],"metadata":{"id":"Vmvn5Qnkz5O_"}},{"cell_type":"code","source":["def format_dataset(eng, spa):\n","    eng = eng_vectorization(eng)\n","    spa = spa_vectorization(spa)\n","    return (\n","        {\n","            'encoder_inputs': eng,\n","            'decoder_inputs': spa[:,:-1]\n","        },\n","        spa[:,:-1]\n","    )\n","\n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.cache().shuffle(2048).prefetch(16)\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"metadata":{"id":"EYGqbS8mz6n7","executionInfo":{"status":"ok","timestamp":1759692573038,"user_tz":180,"elapsed":1215,"user":{"displayName":"Fabrizio Rejala","userId":"01308745419046227170"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Tenemos que tener batches de 64 y en grupos, que van a ser mis secuencias para utilizar en el decoder y encoder"],"metadata":{"id":"hj7OKQwyz8G-"}},{"cell_type":"code","source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f'target.shape: {targets.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRlNfVk9z9WU","executionInfo":{"status":"ok","timestamp":1759692575522,"user_tz":180,"elapsed":906,"user":{"displayName":"Fabrizio Rejala","userId":"01308745419046227170"}},"outputId":"f8999cf3-9db0-4f73-dfe6-61ff686d28dd"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs[\"encoder_inputs\"].shape: (64, 20)\n","inputs[\"decoder_inputs\"].shape: (64, 20)\n","target.shape: (64, 20)\n"]}]},{"cell_type":"markdown","source":["Estamos listos para construir nuestro modelo!\n","\n","Tenemos un sec2sec transformer que necesita reconocer las posición de las frases/oraciones, entonces necesitamos una capa\n","PositionalEmbedding para poder tomar en cuenta el orden de las palabras"],"metadata":{"id":"4XuV3p4P0YGW"}},{"cell_type":"code","source":["import keras.ops as ops\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self,embed_dim,dense_dim,num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        #multihead attention\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads = num_heads,key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(dense_dim, activation = \"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = ops.cast(mask[:,None,:], dtype=\"int32\")\n","        else:\n","            padding_mask = None\n","\n","        attention_output = self.attention(\n","            query=inputs,value=inputs,key=inputs,attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        #capa densa\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"embed_dim\":self.embed_dim,\n","                \"dense_dim\":self.dense_dim,\n","                \"num_heads\":self.num_heads,\n","            }\n","        )\n","        return config\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self,sequence_length, vocab_size, embed_dim,**kwargs):\n","        super().__init__(**kwargs)\n","        #embeddings\n","        self.token_embeddings = layers.Embedding(\n","            input_dim = vocab_size, output_dim = embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim = embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = ops.shape(inputs)[-1]\n","        positions = ops.arange(0,length, 1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return ops.not_equal(inputs,0)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"sequence_length\": self.sequence_length,\n","                \"vocab_size\": self.vocab_size,\n","                \"embed_dim\": self.embed_dim,\n","            }\n","        )\n","        return config\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self,embed_dim,latent_dim,num_heads,**kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        #MultiHead-attention\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads,key_dim = embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads,key_dim = embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(latent_dim,activation=\"relu\"),\n","                layers.Dense(embed_dim)\n","            ]\n","        )\n","        #normalizacion\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        inputs, encoder_outputs = inputs\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","\n","        if mask is None:\n","            inputs_padding_mask,encoder_outputs_padding_mask = None, None\n","        else:\n","            inputs_padding_mask, encoder_outputs_padding_mask = mask\n","\n","        attention_output_1 = self.attention_1(\n","            query = inputs,\n","            value = inputs,\n","            key = inputs,\n","            attention_mask = causal_mask,\n","            query_mask = inputs_padding_mask,\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query = out_1,\n","            value = encoder_outputs,\n","            key = encoder_outputs,\n","            query_mask = inputs_padding_mask,\n","            key_mask = encoder_outputs_padding_mask,\n","        )\n","\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        #capa densa\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","\n","    def get_causal_attention_mask(self,inputs):\n","        input_shape = ops.shape(inputs)\n","        batch_size,sequence_length = input_shape[0], input_shape[1]\n","        i = ops.arange(sequence_length)[:,None]\n","        j = ops.arange(sequence_length)\n","        mask = ops.cast(i >=j , dtype=\"int32\")\n","        mask = ops.reshape(mask, (1, input_shape[1],input_shape[1]))\n","        mult = ops.concatenate(\n","            [ops.expand_dims(batch_size,-1),ops.convert_to_tensor([1,1])],\n","            axis=0,\n","        )\n","        return ops.tile(mask,mult)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"embed_dim\": self.embed_dim,\n","                \"latent_dim\": self.latent_dim,\n","                \"num_heads\": self.num_heads,\n","            }\n","        )\n","        return config"],"metadata":{"id":"HIWACyEN0dWV","executionInfo":{"status":"ok","timestamp":1759692576735,"user_tz":180,"elapsed":14,"user":{"displayName":"Fabrizio Rejala","userId":"01308745419046227170"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size,embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,),dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None,embed_dim), name =\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length,vocab_size,embed_dim)(decoder_inputs)\n","x=TransformerDecoder(embed_dim,latent_dim,num_heads)([x,encoded_seq_inputs])\n","x=layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size,activation=\"softmax\")(x)\n","decoder = keras.Model([decoder_inputs,encoded_seq_inputs],decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs,encoder_outputs])\n","\n","Transformer = keras.Model(\n","    [encoder_inputs,decoder_inputs],\n","    decoder_outputs,\n","    name=\"transformer\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFieUsRZ0e53","executionInfo":{"status":"ok","timestamp":1759692583039,"user_tz":180,"elapsed":1983,"user":{"displayName":"Fabrizio Rejala","userId":"01308745419046227170"}},"outputId":"22a406f5-eac3-4fd5-ebde-ccfc12230e0e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:965: UserWarning: Layer 'functional_3' (of type Functional) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["epochs = 1\n","\n","Transformer.summary()\n","Transformer.compile(\n","    \"rmsprop\",\n","    loss=keras.losses.SparseCategoricalCrossentropy(ignore_class=0),\n","    metrics=[\"accuracy\"],\n",")\n","Transformer.fit(train_ds,epochs=epochs,validation_data=val_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"id":"7LM4A5HW0gRX","outputId":"03490057-0f41-4896-8c73-1b29b4fb68c0"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"transformer\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │ \u001b[38;5;34m38,405,120\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m3,155,456\u001b[0m │ positional_embed… │\n","│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ functional_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │ \u001b[38;5;34m82,214,640\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m150000\u001b[0m)           │            │ transformer_enco… │\n","│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">38,405,120</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,155,456</span> │ positional_embed… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ functional_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">82,214,640</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">150000</span>)           │            │ transformer_enco… │\n","│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m123,775,216\u001b[0m (472.16 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">123,775,216</span> (472.16 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m123,775,216\u001b[0m (472.16 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">123,775,216</span> (472.16 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["spa_vocab = spa_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)),spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","def decoded_sequence(input_sequence):\n","    tokenized_input_sentence = eng_vectorization([input_sequence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n","        predictions = Transformer.predict(\n","            {\n","                \"encoder_inputs\": tokenized_input_sentence,\n","                \"decoder_inputs\": tokenized_target_sentence,\n","            }\n","        )\n","        sampled_token_index = ops.convert_to_numpy(\n","            ops.argmax(predictions[0,i,:])\n","        ).item(0)\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == [\"end\"]:\n","            break\n","    return decoded_sentence\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for i in range(30):\n","    input_sentence = random.choice(test_eng_texts)\n","    translated = decoded_sequence(input_sentence)"],"metadata":{"id":"ITtC0Dva0iZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_eng_texts = [pair[0] for pair in test_pairs]\n","test_spa_texts = [pair[1] for pair in test_pairs]\n","\n","print(\"Traducciones de selecciones random\")\n","print(\"-\"*50)\n","\n","for i in range(30):\n","    idx = random.randint(0, len(test_eng_texts)-1)\n","    input_sentence = test_eng_texts[idx]\n","    reference_translation = test_spa_texts[idx].replace(\"[start] \",\"\").replace(\" [end]\",\"\")\n","\n","    predicted_translation = decoded_sequence(input_sentence)\n","    predicted_translation = predicted_translation.replace(\"[start] \",\"\").replace(\" [end]\",\"\")\n","\n","    print(f\"English:{input_sentence}\")\n","    print(f\"Ref: {reference_translation}\")\n","    print(f\"Español: {predicted_translation}\")"],"metadata":{"id":"fzpdoRML0kLN"},"execution_count":null,"outputs":[]}]}